use crate::token::TokenType;
use pyo3::prelude::*;
use std::collections::{HashMap, HashSet};
use std::str::FromStr;

#[derive(Clone, Debug)]
#[pyclass]
pub struct TokenizerSettings {
    pub white_space: HashMap<char, TokenType>,
    pub single_tokens: HashMap<char, TokenType>,
    pub keywords: HashMap<String, TokenType>,
    pub numeric_literals: HashMap<String, String>,
    pub identifiers: HashMap<char, char>,
    pub identifier_escapes: HashSet<char>,
    pub string_escapes: HashSet<char>,
    pub escape_sequences: HashMap<String, String>,
    pub quotes: HashMap<String, String>,
    pub format_strings: HashMap<String, (String, TokenType)>,
    pub has_bit_strings: bool,
    pub has_hex_strings: bool,
    pub comments: HashMap<String, Option<String>>,
    pub var_single_tokens: HashSet<char>,
    pub commands: HashSet<TokenType>,
    pub command_prefix_tokens: HashSet<TokenType>,
    pub identifiers_can_start_with_digit: bool,
}

impl TokenizerSettings {
    pub fn default() -> TokenizerSettings {
        TokenizerSettings {
            white_space: HashMap::from([
                (' ', TokenType::SPACE),
                ('\t', TokenType::SPACE),
                ('\n', TokenType::BREAK),
                ('\r', TokenType::BREAK),
            ]),
            single_tokens: HashMap::from([
                ('(', TokenType::L_PAREN),
                (')', TokenType::R_PAREN),
                ('[', TokenType::L_BRACKET),
                (']', TokenType::R_BRACKET),
                ('{', TokenType::L_BRACE),
                ('}', TokenType::R_BRACE),
                ('&', TokenType::AMP),
                ('^', TokenType::CARET),
                (':', TokenType::COLON),
                (',', TokenType::COMMA),
                ('.', TokenType::DOT),
                ('-', TokenType::DASH),
                ('=', TokenType::EQ),
                ('>', TokenType::GT),
                ('<', TokenType::LT),
                ('%', TokenType::MOD),
                ('!', TokenType::NOT),
                ('|', TokenType::PIPE),
                ('+', TokenType::PLUS),
                (';', TokenType::SEMICOLON),
                ('/', TokenType::SLASH),
                ('\\', TokenType::BACKSLASH),
                ('*', TokenType::STAR),
                ('~', TokenType::TILDA),
                ('?', TokenType::PLACEHOLDER),
                ('@', TokenType::PARAMETER),
                ('\'', TokenType::QUOTE),
                ('`', TokenType::IDENTIFIER),
                ('"', TokenType::IDENTIFIER),
                ('#', TokenType::HASH),
            ]),
            keywords: HashMap::from([
                ("{%".to_string(), TokenType::BLOCK_START),
                ("{%+".to_string(), TokenType::BLOCK_START),
                ("{%-".to_string(), TokenType::BLOCK_START),
                ("%}".to_string(), TokenType::BLOCK_END),
                ("+%}".to_string(), TokenType::BLOCK_END),
                ("-%}".to_string(), TokenType::BLOCK_END),
                ("{{+".to_string(), TokenType::BLOCK_START),
                ("{{-".to_string(), TokenType::BLOCK_START),
                ("+}}".to_string(), TokenType::BLOCK_END),
                ("-}}".to_string(), TokenType::BLOCK_END),
                ("/*+".to_string(), TokenType::HINT),
                ("==".to_string(), TokenType::EQ),
                ("::".to_string(), TokenType::DCOLON),
                ("||".to_string(), TokenType::DPIPE),
                (">=".to_string(), TokenType::GTE),
                ("<=".to_string(), TokenType::LTE),
                ("<>".to_string(), TokenType::NEQ),
                ("!=".to_string(), TokenType::NEQ),
                ("<=>".to_string(), TokenType::NULLSAFE_EQ),
                ("->".to_string(), TokenType::ARROW),
                ("->>".to_string(), TokenType::DARROW),
                ("=>".to_string(), TokenType::FARROW),
                ("#>".to_string(), TokenType::HASH_ARROW),
                ("#>>".to_string(), TokenType::DHASH_ARROW),
                ("<->".to_string(), TokenType::LR_ARROW),
                ("&&".to_string(), TokenType::DAMP),
                ("??".to_string(), TokenType::DQMARK),
                ("ALL".to_string(), TokenType::ALL),
                ("ALWAYS".to_string(), TokenType::ALWAYS),
                ("AND".to_string(), TokenType::AND),
                ("ANTI".to_string(), TokenType::ANTI),
                ("ANY".to_string(), TokenType::ANY),
                ("ASC".to_string(), TokenType::ASC),
                ("AS".to_string(), TokenType::ALIAS),
                ("ASOF".to_string(), TokenType::ASOF),
                ("AUTOINCREMENT".to_string(), TokenType::AUTO_INCREMENT),
                ("AUTO_INCREMENT".to_string(), TokenType::AUTO_INCREMENT),
                ("BEGIN".to_string(), TokenType::BEGIN),
                ("BETWEEN".to_string(), TokenType::BETWEEN),
                ("CACHE".to_string(), TokenType::CACHE),
                ("UNCACHE".to_string(), TokenType::UNCACHE),
                ("CASE".to_string(), TokenType::CASE),
                ("CHARACTER SET".to_string(), TokenType::CHARACTER_SET),
                ("CLUSTER BY".to_string(), TokenType::CLUSTER_BY),
                ("COLLATE".to_string(), TokenType::COLLATE),
                ("COLUMN".to_string(), TokenType::COLUMN),
                ("COMMIT".to_string(), TokenType::COMMIT),
                ("CONNECT BY".to_string(), TokenType::CONNECT_BY),
                ("CONSTRAINT".to_string(), TokenType::CONSTRAINT),
                ("CREATE".to_string(), TokenType::CREATE),
                ("CROSS".to_string(), TokenType::CROSS),
                ("CUBE".to_string(), TokenType::CUBE),
                ("CURRENT_DATE".to_string(), TokenType::CURRENT_DATE),
                ("CURRENT_TIME".to_string(), TokenType::CURRENT_TIME),
                (
                    "CURRENT_TIMESTAMP".to_string(),
                    TokenType::CURRENT_TIMESTAMP,
                ),
                ("CURRENT_USER".to_string(), TokenType::CURRENT_USER),
                ("DATABASE".to_string(), TokenType::DATABASE),
                ("DEFAULT".to_string(), TokenType::DEFAULT),
                ("DELETE".to_string(), TokenType::DELETE),
                ("DESC".to_string(), TokenType::DESC),
                ("DESCRIBE".to_string(), TokenType::DESCRIBE),
                ("DISTINCT".to_string(), TokenType::DISTINCT),
                ("DISTRIBUTE BY".to_string(), TokenType::DISTRIBUTE_BY),
                ("DIV".to_string(), TokenType::DIV),
                ("DROP".to_string(), TokenType::DROP),
                ("ELSE".to_string(), TokenType::ELSE),
                ("END".to_string(), TokenType::END),
                ("ESCAPE".to_string(), TokenType::ESCAPE),
                ("EXCEPT".to_string(), TokenType::EXCEPT),
                ("EXECUTE".to_string(), TokenType::EXECUTE),
                ("EXISTS".to_string(), TokenType::EXISTS),
                ("FALSE".to_string(), TokenType::FALSE),
                ("FETCH".to_string(), TokenType::FETCH),
                ("FILTER".to_string(), TokenType::FILTER),
                ("FIRST".to_string(), TokenType::FIRST),
                ("FULL".to_string(), TokenType::FULL),
                ("FUNCTION".to_string(), TokenType::FUNCTION),
                ("FOR".to_string(), TokenType::FOR),
                ("FOREIGN KEY".to_string(), TokenType::FOREIGN_KEY),
                ("FORMAT".to_string(), TokenType::FORMAT),
                ("FROM".to_string(), TokenType::FROM),
                ("GEOGRAPHY".to_string(), TokenType::GEOGRAPHY),
                ("GEOMETRY".to_string(), TokenType::GEOMETRY),
                ("GLOB".to_string(), TokenType::GLOB),
                ("GROUP BY".to_string(), TokenType::GROUP_BY),
                ("GROUPING SETS".to_string(), TokenType::GROUPING_SETS),
                ("HAVING".to_string(), TokenType::HAVING),
                ("ILIKE".to_string(), TokenType::ILIKE),
                ("IN".to_string(), TokenType::IN),
                ("INDEX".to_string(), TokenType::INDEX),
                ("INET".to_string(), TokenType::INET),
                ("INNER".to_string(), TokenType::INNER),
                ("INSERT".to_string(), TokenType::INSERT),
                ("INTERVAL".to_string(), TokenType::INTERVAL),
                ("INTERSECT".to_string(), TokenType::INTERSECT),
                ("INTO".to_string(), TokenType::INTO),
                ("IS".to_string(), TokenType::IS),
                ("ISNULL".to_string(), TokenType::ISNULL),
                ("JOIN".to_string(), TokenType::JOIN),
                ("KEEP".to_string(), TokenType::KEEP),
                ("KILL".to_string(), TokenType::KILL),
                ("LATERAL".to_string(), TokenType::LATERAL),
                ("LEFT".to_string(), TokenType::LEFT),
                ("LIKE".to_string(), TokenType::LIKE),
                ("LIMIT".to_string(), TokenType::LIMIT),
                ("LOAD".to_string(), TokenType::LOAD),
                ("LOCK".to_string(), TokenType::LOCK),
                ("MERGE".to_string(), TokenType::MERGE),
                ("NATURAL".to_string(), TokenType::NATURAL),
                ("NEXT".to_string(), TokenType::NEXT),
                ("NOT".to_string(), TokenType::NOT),
                ("NOTNULL".to_string(), TokenType::NOTNULL),
                ("NULL".to_string(), TokenType::NULL),
                ("OBJECT".to_string(), TokenType::OBJECT),
                ("OFFSET".to_string(), TokenType::OFFSET),
                ("ON".to_string(), TokenType::ON),
                ("OR".to_string(), TokenType::OR),
                ("XOR".to_string(), TokenType::XOR),
                ("ORDER BY".to_string(), TokenType::ORDER_BY),
                ("ORDINALITY".to_string(), TokenType::ORDINALITY),
                ("OUTER".to_string(), TokenType::OUTER),
                ("OVER".to_string(), TokenType::OVER),
                ("OVERLAPS".to_string(), TokenType::OVERLAPS),
                ("OVERWRITE".to_string(), TokenType::OVERWRITE),
                ("PARTITION".to_string(), TokenType::PARTITION),
                ("PARTITION BY".to_string(), TokenType::PARTITION_BY),
                ("PARTITIONED BY".to_string(), TokenType::PARTITION_BY),
                ("PARTITIONED_BY".to_string(), TokenType::PARTITION_BY),
                ("PERCENT".to_string(), TokenType::PERCENT),
                ("PIVOT".to_string(), TokenType::PIVOT),
                ("PRAGMA".to_string(), TokenType::PRAGMA),
                ("PRIMARY KEY".to_string(), TokenType::PRIMARY_KEY),
                ("PROCEDURE".to_string(), TokenType::PROCEDURE),
                ("QUALIFY".to_string(), TokenType::QUALIFY),
                ("RANGE".to_string(), TokenType::RANGE),
                ("RECURSIVE".to_string(), TokenType::RECURSIVE),
                ("REGEXP".to_string(), TokenType::RLIKE),
                ("REPLACE".to_string(), TokenType::REPLACE),
                ("RETURNING".to_string(), TokenType::RETURNING),
                ("REFERENCES".to_string(), TokenType::REFERENCES),
                ("RIGHT".to_string(), TokenType::RIGHT),
                ("RLIKE".to_string(), TokenType::RLIKE),
                ("ROLLBACK".to_string(), TokenType::ROLLBACK),
                ("ROLLUP".to_string(), TokenType::ROLLUP),
                ("ROW".to_string(), TokenType::ROW),
                ("ROWS".to_string(), TokenType::ROWS),
                ("SCHEMA".to_string(), TokenType::SCHEMA),
                ("SELECT".to_string(), TokenType::SELECT),
                ("SEMI".to_string(), TokenType::SEMI),
                ("SET".to_string(), TokenType::SET),
                ("SETTINGS".to_string(), TokenType::SETTINGS),
                ("SHOW".to_string(), TokenType::SHOW),
                ("SIMILAR TO".to_string(), TokenType::SIMILAR_TO),
                ("SOME".to_string(), TokenType::SOME),
                ("SORT BY".to_string(), TokenType::SORT_BY),
                ("START WITH".to_string(), TokenType::START_WITH),
                ("TABLE".to_string(), TokenType::TABLE),
                ("TABLESAMPLE".to_string(), TokenType::TABLE_SAMPLE),
                ("TEMP".to_string(), TokenType::TEMPORARY),
                ("TEMPORARY".to_string(), TokenType::TEMPORARY),
                ("THEN".to_string(), TokenType::THEN),
                ("TRUE".to_string(), TokenType::TRUE),
                ("UNION".to_string(), TokenType::UNION),
                ("UNKNOWN".to_string(), TokenType::UNKNOWN),
                ("UNNEST".to_string(), TokenType::UNNEST),
                ("UNPIVOT".to_string(), TokenType::UNPIVOT),
                ("UPDATE".to_string(), TokenType::UPDATE),
                ("USE".to_string(), TokenType::USE),
                ("USING".to_string(), TokenType::USING),
                ("UUID".to_string(), TokenType::UUID),
                ("VALUES".to_string(), TokenType::VALUES),
                ("VIEW".to_string(), TokenType::VIEW),
                ("VOLATILE".to_string(), TokenType::VOLATILE),
                ("WHEN".to_string(), TokenType::WHEN),
                ("WHERE".to_string(), TokenType::WHERE),
                ("WINDOW".to_string(), TokenType::WINDOW),
                ("WITH".to_string(), TokenType::WITH),
                ("APPLY".to_string(), TokenType::APPLY),
                ("ARRAY".to_string(), TokenType::ARRAY),
                ("BIT".to_string(), TokenType::BIT),
                ("BOOL".to_string(), TokenType::BOOLEAN),
                ("BOOLEAN".to_string(), TokenType::BOOLEAN),
                ("BYTE".to_string(), TokenType::TINYINT),
                ("MEDIUMINT".to_string(), TokenType::MEDIUMINT),
                ("TINYINT".to_string(), TokenType::TINYINT),
                ("SHORT".to_string(), TokenType::SMALLINT),
                ("SMALLINT".to_string(), TokenType::SMALLINT),
                ("INT128".to_string(), TokenType::INT128),
                ("INT2".to_string(), TokenType::SMALLINT),
                ("INTEGER".to_string(), TokenType::INT),
                ("INT".to_string(), TokenType::INT),
                ("INT4".to_string(), TokenType::INT),
                ("LONG".to_string(), TokenType::BIGINT),
                ("BIGINT".to_string(), TokenType::BIGINT),
                ("INT8".to_string(), TokenType::BIGINT),
                ("DEC".to_string(), TokenType::DECIMAL),
                ("DECIMAL".to_string(), TokenType::DECIMAL),
                ("BIGDECIMAL".to_string(), TokenType::BIGDECIMAL),
                ("BIGNUMERIC".to_string(), TokenType::BIGDECIMAL),
                ("MAP".to_string(), TokenType::MAP),
                ("NULLABLE".to_string(), TokenType::NULLABLE),
                ("NUMBER".to_string(), TokenType::DECIMAL),
                ("NUMERIC".to_string(), TokenType::DECIMAL),
                ("FIXED".to_string(), TokenType::DECIMAL),
                ("REAL".to_string(), TokenType::FLOAT),
                ("FLOAT".to_string(), TokenType::FLOAT),
                ("FLOAT4".to_string(), TokenType::FLOAT),
                ("FLOAT8".to_string(), TokenType::DOUBLE),
                ("DOUBLE".to_string(), TokenType::DOUBLE),
                ("DOUBLE PRECISION".to_string(), TokenType::DOUBLE),
                ("JSON".to_string(), TokenType::JSON),
                ("CHAR".to_string(), TokenType::CHAR),
                ("CHARACTER".to_string(), TokenType::CHAR),
                ("NCHAR".to_string(), TokenType::NCHAR),
                ("VARCHAR".to_string(), TokenType::VARCHAR),
                ("VARCHAR2".to_string(), TokenType::VARCHAR),
                ("NVARCHAR".to_string(), TokenType::NVARCHAR),
                ("NVARCHAR2".to_string(), TokenType::NVARCHAR),
                ("STR".to_string(), TokenType::TEXT),
                ("STRING".to_string(), TokenType::TEXT),
                ("TEXT".to_string(), TokenType::TEXT),
                ("LONGTEXT".to_string(), TokenType::LONGTEXT),
                ("MEDIUMTEXT".to_string(), TokenType::MEDIUMTEXT),
                ("TINYTEXT".to_string(), TokenType::TINYTEXT),
                ("CLOB".to_string(), TokenType::TEXT),
                ("LONGVARCHAR".to_string(), TokenType::TEXT),
                ("BINARY".to_string(), TokenType::BINARY),
                ("BLOB".to_string(), TokenType::VARBINARY),
                ("LONGBLOB".to_string(), TokenType::LONGBLOB),
                ("MEDIUMBLOB".to_string(), TokenType::MEDIUMBLOB),
                ("TINYBLOB".to_string(), TokenType::TINYBLOB),
                ("BYTEA".to_string(), TokenType::VARBINARY),
                ("VARBINARY".to_string(), TokenType::VARBINARY),
                ("TIME".to_string(), TokenType::TIME),
                ("TIMETZ".to_string(), TokenType::TIMETZ),
                ("TIMESTAMP".to_string(), TokenType::TIMESTAMP),
                ("TIMESTAMPTZ".to_string(), TokenType::TIMESTAMPTZ),
                ("TIMESTAMPLTZ".to_string(), TokenType::TIMESTAMPLTZ),
                ("DATE".to_string(), TokenType::DATE),
                ("DATETIME".to_string(), TokenType::DATETIME),
                ("INT4RANGE".to_string(), TokenType::INT4RANGE),
                ("INT4MULTIRANGE".to_string(), TokenType::INT4MULTIRANGE),
                ("INT8RANGE".to_string(), TokenType::INT8RANGE),
                ("INT8MULTIRANGE".to_string(), TokenType::INT8MULTIRANGE),
                ("NUMRANGE".to_string(), TokenType::NUMRANGE),
                ("NUMMULTIRANGE".to_string(), TokenType::NUMMULTIRANGE),
                ("TSRANGE".to_string(), TokenType::TSRANGE),
                ("TSMULTIRANGE".to_string(), TokenType::TSMULTIRANGE),
                ("TSTZRANGE".to_string(), TokenType::TSTZRANGE),
                ("TSTZMULTIRANGE".to_string(), TokenType::TSTZMULTIRANGE),
                ("DATERANGE".to_string(), TokenType::DATERANGE),
                ("DATEMULTIRANGE".to_string(), TokenType::DATEMULTIRANGE),
                ("UNIQUE".to_string(), TokenType::UNIQUE),
                ("STRUCT".to_string(), TokenType::STRUCT),
                ("VARIANT".to_string(), TokenType::VARIANT),
                ("ALTER".to_string(), TokenType::ALTER),
                ("ANALYZE".to_string(), TokenType::COMMAND),
                ("CALL".to_string(), TokenType::COMMAND),
                ("COMMENT".to_string(), TokenType::COMMENT),
                ("COPY".to_string(), TokenType::COMMAND),
                ("EXPLAIN".to_string(), TokenType::COMMAND),
                ("GRANT".to_string(), TokenType::COMMAND),
                ("OPTIMIZE".to_string(), TokenType::COMMAND),
                ("PREPARE".to_string(), TokenType::COMMAND),
                ("TRUNCATE".to_string(), TokenType::COMMAND),
                ("VACUUM".to_string(), TokenType::COMMAND),
                ("USER-DEFINED".to_string(), TokenType::USERDEFINED),
                ("FOR VERSION".to_string(), TokenType::VERSION_SNAPSHOT),
                ("FOR TIMESTAMP".to_string(), TokenType::TIMESTAMP_SNAPSHOT),
            ]),
            numeric_literals: HashMap::new(),
            identifiers: HashMap::from([('"', '"')]),
            identifier_escapes: HashSet::from(['"']),
            string_escapes: HashSet::from(['\'']),
            escape_sequences: HashMap::new(),
            quotes: HashMap::from([("'".to_string(), "'".to_string())]),
            format_strings: HashMap::new(),
            has_bit_strings: false,
            has_hex_strings: false,
            comments: HashMap::from([
                ("--".to_string(), None),
                ("/*".to_string(), Some("*/".to_string())),
            ]),
            var_single_tokens: HashSet::new(),
            commands: HashSet::from([
                TokenType::COMMAND,
                TokenType::EXECUTE,
                TokenType::FETCH,
                TokenType::SHOW,
            ]),
            command_prefix_tokens: HashSet::from([TokenType::SEMICOLON, TokenType::BEGIN]),
            identifiers_can_start_with_digit: false,
        }
    }
}

#[pymethods]
impl TokenizerSettings {
    #[new]
    pub fn new(
        white_space: HashMap<String, String>,
        single_tokens: HashMap<String, String>,
        keywords: HashMap<String, String>,
        numeric_literals: HashMap<String, String>,
        identifiers: HashMap<String, String>,
        identifier_escapes: HashSet<String>,
        string_escapes: HashSet<String>,
        escape_sequences: HashMap<String, String>,
        quotes: HashMap<String, String>,
        format_strings: HashMap<String, (String, String)>,
        has_bit_strings: bool,
        has_hex_strings: bool,
        comments: HashMap<String, Option<String>>,
        var_single_tokens: HashSet<String>,
        commands: HashSet<String>,
        command_prefix_tokens: HashSet<String>,
        identifiers_can_start_with_digit: bool,
    ) -> Self {
        let to_token_type = |v: &String| match TokenType::from_str(v) {
            Ok(t) => t,
            Err(_) => panic!("Invalid token type: {}", v),
        };

        let to_char = |v: &String| {
            if v.len() == 1 {
                v.chars().next().unwrap()
            } else {
                panic!("Invalid char: {}", v)
            }
        };

        let white_space_native: HashMap<char, TokenType> = white_space
            .iter()
            .map(|(k, v)| (to_char(k), to_token_type(v)))
            .collect();

        let single_tokens_native: HashMap<char, TokenType> = single_tokens
            .iter()
            .map(|(k, v)| (to_char(k), to_token_type(v)))
            .collect();

        let keywords_native: HashMap<String, TokenType> = keywords
            .into_iter()
            .map(|(k, v)| (k, to_token_type(&v)))
            .collect();

        let identifiers_native: HashMap<char, char> = identifiers
            .iter()
            .map(|(k, v)| (to_char(k), to_char(v)))
            .collect();

        let identifier_escapes_native: HashSet<char> =
            identifier_escapes.iter().map(&to_char).collect();

        let string_escapes_native: HashSet<char> = string_escapes.iter().map(&to_char).collect();

        let format_strings_native: HashMap<String, (String, TokenType)> = format_strings
            .into_iter()
            .map(|(k, (v1, v2))| (k, (v1, to_token_type(&v2))))
            .collect();

        let var_single_tokens_native: HashSet<char> =
            var_single_tokens.iter().map(&to_char).collect();

        let commands_native: HashSet<TokenType> = commands.iter().map(&to_token_type).collect();

        let command_prefix_tokens_native: HashSet<TokenType> =
            command_prefix_tokens.iter().map(&to_token_type).collect();

        TokenizerSettings {
            white_space: white_space_native,
            single_tokens: single_tokens_native,
            keywords: keywords_native,
            numeric_literals,
            identifiers: identifiers_native,
            identifier_escapes: identifier_escapes_native,
            string_escapes: string_escapes_native,
            escape_sequences,
            quotes,
            format_strings: format_strings_native,
            has_bit_strings,
            has_hex_strings,
            comments,
            var_single_tokens: var_single_tokens_native,
            commands: commands_native,
            command_prefix_tokens: command_prefix_tokens_native,
            identifiers_can_start_with_digit,
        }
    }
}
